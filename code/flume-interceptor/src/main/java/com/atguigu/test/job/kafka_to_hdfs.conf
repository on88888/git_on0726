#1、定义agent、source、channel、sink的名称
a1.sources=r1
a1.sinks=k1
a1.channels=c1

#2、描述source
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
#指定kafka集群地址
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092
#指定从哪个topic拉取数据
a1.sources.r1.kafka.topics = applog
#指定消费者组的id
a1.sources.r1.kafka.consumer.group.id = log1
#设置source拉取数据的批次大小
a1.sources.r1.batchSize = 100
#设置是否以event数据格式读取数据
a1.sources.r1.useFlumeEventFormat = false
#指定消费者组第一次消费topic数据的时候从哪个问题开始消费
a1.sources.r1.kafka.consumer.auto.offset.reset = earliest

#3、描述拦截器[处理数据漂移]
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.atguigu.interceptor.MyTimestampInterceptor$Builder

#4、描述channel
a1.channels.c1.type = file
#指定file channel保存数据的路径
a1.channels.c1.dataDirs = /opt/module/flume/datas
#指定内存指针的保存路径
a1.channels.c1.checkpointDir = /opt/module/flume/checkpoints
#指定事务的容量大小 batchSize<=transactionCapacity<=capacity
a1.channels.c1.transactionCapacity = 1000
#指定channel的容量大小
a1.channels.c1.capacity = 1000000

#5、描述sink
a1.sinks.k1.type = hdfs
#指定数据存储目录
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
#指定文件前缀
a1.sinks.k1.hdfs.filePrefix = log-
#指定flume向文件中写多久之后,生成一个新文件,后续数据向新文件中写入,老文件不再写入数据
a1.sinks.k1.hdfs.rollInterval = 30
#指定flume向文件中写多大的数据量之后,生成一个新文件,后续数据向新文件中写入,老文件不再写入数据
a1.sinks.k1.hdfs.rollSize = 132120576
#指定flume向文件中写多少个event之后,生成一个新文件,后续数据向新文件中写入,老文件不再写入数据
a1.sinks.k1.hdfs.rollCount = 0
#指定sink拉取channel的批次大小
a1.sinks.k1.hdfs.batchSize = 100
#指定数据保存的文件类型[SequenceFile-序列文件, DataStream-文本, CompressedStream-压缩文件]
a1.sinks.k1.hdfs.fileType = CompressedStream
#指定压缩格式
a1.sinks.k1.hdfs.codeC = lzop

#6、关联source->channel->sink
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1